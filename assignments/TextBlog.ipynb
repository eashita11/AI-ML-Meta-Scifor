{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5872a9db",
   "metadata": {},
   "source": [
    "## Text Blob: Simplified Text Processing\n",
    "TextBlob is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\\\\\n",
    "\n",
    "TextBlob stands on the giant shoulders of NLTK and pattern and plays nicely with both.\n",
    "\n",
    "Features\n",
    "* Noun phrase extraction\n",
    "* Part-of-speech tagging\n",
    "* Sentiment analysis\n",
    "* Classification (Naive Bayes, Decision Tree)\n",
    "* Language translation and detection powered by Google Translate\n",
    "* Tokenization (splitting text into words and sentences)\n",
    "* Word and phrase frequencies\n",
    "* Parsing\n",
    "* N-grams\n",
    "* Word inflection (pluralization and singularization) and lemmatization\n",
    "* Spelling correction\n",
    "* Add new models or languages through extensions\n",
    "* WordNet integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa47c95e",
   "metadata": {},
   "source": [
    "## Installation\n",
    "### Installing/Uploading from the PyPi\n",
    "    pip install textblog\n",
    "    python -m textblog.download_corpora\n",
    "This will install TextBlob and download the necessary NLTK corpora. If you need to change the default download directory, set the NLTK_DATA environment variable.\n",
    "\n",
    "### Downloading the minimum corpora:\n",
    "If you only intend to use TextBlob’s default models (no model overrides), you can pass the lite argument. This downloads only those corpora needed for basic functionality.\n",
    "\n",
    "    python -m textblob.download_corpora lite \n",
    "    \n",
    "### Installing with Conda\n",
    "Note: Conda builds are currently available for Mac OSX only.\n",
    "\n",
    "TextBlob is also available as a conda package. To install with conda, run:\n",
    "\n",
    "    conda install -c https://conda.anaconda.org/sloria textblob\n",
    "    python -m textblob.download_corpora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3147c5f1",
   "metadata": {},
   "source": [
    "## Python\n",
    "TextBlob supports Python >= 2.7 or >= 3.4.\n",
    "\n",
    "### Dependencies\n",
    "TextBlob depends on NLTK3. NLTK will be installed automatically when you run:\n",
    "\n",
    "    pip install textblob\n",
    "TextBlob aims to provide access to common text-processing operations through a familiar interface. You can treat TextBlob objects as if they were Python strings that learned how to do Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9e6ffa",
   "metadata": {},
   "source": [
    "## Create a TextBlob\n",
    "First, the import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7639d24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in ./anaconda3/lib/python3.11/site-packages (0.18.0.post0)\r\n",
      "Requirement already satisfied: nltk>=3.8 in ./.local/lib/python3.11/site-packages (from textblob) (3.9.1)\r\n",
      "Requirement already satisfied: click in ./anaconda3/lib/python3.11/site-packages (from nltk>=3.8->textblob) (8.0.4)\r\n",
      "Requirement already satisfied: joblib in ./anaconda3/lib/python3.11/site-packages (from nltk>=3.8->textblob) (1.2.0)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./anaconda3/lib/python3.11/site-packages (from nltk>=3.8->textblob) (2022.7.9)\r\n",
      "Requirement already satisfied: tqdm in ./anaconda3/lib/python3.11/site-packages (from nltk>=3.8->textblob) (4.65.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0147b256",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cac6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "387d65a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/anil/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/anil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/anil/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/anil/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to /Users/anil/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/anil/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9c3e61",
   "metadata": {},
   "source": [
    "Let's create our first **TextBlob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6345da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = TextBlob(\"I love Natural Language Processing, not you!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7106a30",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging\n",
    "Parts-of-speech tags can be accessed through the tags property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92f0bab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('love', 'VBP'),\n",
       " ('Natural', 'JJ'),\n",
       " ('Language', 'NNP'),\n",
       " ('Processing', 'NNP'),\n",
       " ('not', 'RB'),\n",
       " ('you', 'PRP')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932ac846",
   "metadata": {},
   "source": [
    "## Noun Phrase Extraction\n",
    "Similarly, noun phrases are accessed through the noun_phrases property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f822f921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['language processing'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e8974e",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "The sentiment property returns a named tuple of the form Sentiment(polarity, subjectivity). The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0], where 0.0 is very subjective and 1.0 is very subjective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "826344b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.39166666666666666, subjectivity=0.4357142857142857)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testimonial = TextBlob(\"TextBlob is amazingly simple to use. What great fun!\")\n",
    "testimonial.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b5d3b9",
   "metadata": {},
   "source": [
    "### Analyze subjectivity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c88cd709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4357142857142857"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testimonial.sentiment.subjectivity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d4d2ba",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "311fa7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Data', 'is', 'a', 'new', 'fuel', 'Explicit', 'is', 'better', 'than', 'implicit', 'Simple', 'is', 'better', 'than', 'complex'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen = TextBlob(\"Data is a new fuel. \"\n",
    "               \"Explicit is better than implicit. \"\n",
    "               \"Simple is better than complex. \")\n",
    "zen.words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30c195",
   "metadata": {},
   "source": [
    "### Get Sentences:\n",
    "Sentences objects have the same properties and methods as **Textblobs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0702a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"Data is a new fuel.\"),\n",
       " Sentence(\"Explicit is better than implicit.\"),\n",
       " Sentence(\"Simple is better than complex.\")]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e2f615",
   "metadata": {},
   "source": [
    "### Printing each sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1bc0f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is a new fuel.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n"
     ]
    }
   ],
   "source": [
    "for sentence in zen.sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f683f63",
   "metadata": {},
   "source": [
    "## Word Inflection and Lemmatization\n",
    "Each word in the **TextBlob.words** or **Sentence.words** is a **Word** object (a sublclass of unicode) with useful methods, ex: word inflection.\n",
    "### Extract words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b7b5dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Use', '4', 'spaces', 'per', 'indentation', 'level'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = TextBlob(\"Use 4 spaces per indentation level\")\n",
    "sentence.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99e1656d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'space'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.words[2].singularize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "291e1587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Uses'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.words[0].pluralize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a3fffd",
   "metadata": {},
   "source": [
    "#### Words can be lemmatized just by calling the *lemmatize* method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02d7629f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lion'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "q = Word('lions')\n",
    "q.lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dadb0707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q=Word(\"went\")\n",
    "q.lemmatize(\"v\") # Pass in WordNet part of speech (verb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f00c53e",
   "metadata": {},
   "source": [
    "## WordNet Integration\n",
    "You can access the synsets for a Word via the synsets property or the get_synsets method, optionally passing in a parts-of-speech.\n",
    "\n",
    "### WordNet\n",
    "WordNet is a lexical database that is a dictionary for the English language. It is specifically for natural language processing.\n",
    "\n",
    "### Synset\n",
    "It is a special kind of simple interface that is present in the NLTK for looking up words in WordNet. Synset instances are the groupings of synonymous words that express the same type of concept. Some words have only one synset and some have several.\n",
    "\n",
    "You can access the definitions for each synset via the definitions property or the define() method, which can also take an optional part-of-speech (POS) argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7564198c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the linear extent in space from one end to the other; the longest dimension of something that is fixed in place',\n",
       " 'continuance in time',\n",
       " 'the property of being the extent of something from beginning to end',\n",
       " 'size of the gap between two places',\n",
       " 'a section of something that is long and narrow']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word(\"length\").definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf868974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1111111111111111"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob.wordnet import Synset\n",
    "octopus = Synset('octopus.n.02')\n",
    "shrimp = Synset('shrimp.n.03')\n",
    "octopus.path_similarity(shrimp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a7db5",
   "metadata": {},
   "source": [
    "## WordLists\n",
    "A wordlist is just the Python list with additional methods.\n",
    "\n",
    "WordLists will find the words which are in the sentence and ignore the spaces in between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1f29784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['cow', 'sheep', 'octopus'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals = TextBlob(\"cow sheep octopus\")\n",
    "animals.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36260fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['kine', 'sheep', 'octopodes'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.words.pluralize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d206251",
   "metadata": {},
   "source": [
    "## Spelling Correction\n",
    "For correcting the words, you can use correct() method to attempt spelling correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "783236e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " An you pronounce czechoslovakia?\n"
     ]
    }
   ],
   "source": [
    "g=TextBlob(\" Can you pronounce czechsluovakia?\")\n",
    "print(g.correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a1d9b9",
   "metadata": {},
   "source": [
    "Word objects have a spellcheck() method. This method returns a list of (word, confidence) tuples with spelling suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5a5a212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('longitude', 1.0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "k = Word('longitutde')\n",
    "k.spellcheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303857c",
   "metadata": {},
   "source": [
    "This spelling correction is based on Peter Norvig's \"How to Write a Spelling Corrector\", as implemented in the pattern library. It is about 70% accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9b1c4",
   "metadata": {},
   "source": [
    "## Get Word and Noun Phrase Frequencies\n",
    "There are two ways to get the frequency of a word or noun phrase in the TextBlob.\n",
    "\n",
    "The first one is through the word_counts dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9cb553a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = TextBlob('She sales sea shells at the sea shore.')\n",
    "sent.word_counts['sea']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152599ce",
   "metadata": {},
   "source": [
    "If you access the frequencies this way, the search will not be case sensitive, and words that are not found will have a frequency of 0.\n",
    "\n",
    "The second way is to use the count() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ca66f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.words.count('sea')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c8db8",
   "metadata": {},
   "source": [
    "You can specify whether or not the search should be case-sensitive (default is False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "182cef1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.words.count('Sea', case_sensitive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386051cd",
   "metadata": {},
   "source": [
    "In the above example, we have given 'Sea', and of course, 'Sea' is not available in the sentence. 'Sea' is available in lowercase, so it is shown as 0 as a result.\n",
    "Each of these methods can also be used with noun phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f69662e",
   "metadata": {},
   "source": [
    "## Translation and Language Detection\n",
    "TextBlobs can be translated between languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "302ad1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in ./anaconda3/lib/python3.11/site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in ./.local/lib/python3.11/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in ./anaconda3/lib/python3.11/site-packages (from nltk>=3.8->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in ./anaconda3/lib/python3.11/site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./anaconda3/lib/python3.11/site-packages (from nltk>=3.8->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in ./anaconda3/lib/python3.11/site-packages (from nltk>=3.8->textblob) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92ea2291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googletrans==4.0.0-rc1 in ./anaconda3/lib/python3.11/site-packages (4.0.0rc1)\n",
      "Requirement already satisfied: httpx==0.13.3 in ./anaconda3/lib/python3.11/site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: certifi in ./anaconda3/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.7.22)\n",
      "Requirement already satisfied: hstspreload in ./anaconda3/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.10.1)\n",
      "Requirement already satisfied: sniffio in ./anaconda3/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.2.0)\n",
      "Requirement already satisfied: chardet==3.* in ./anaconda3/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in ./anaconda3/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in ./anaconda3/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in ./anaconda3/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in ./anaconda3/lib/python3.11/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in ./anaconda3/lib/python3.11/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in ./anaconda3/lib/python3.11/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in ./anaconda3/lib/python3.11/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cf6a0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "कुछ होना कुछ नहीं होने से बेहतर है।\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "translation = translator.translate('Something is better than nothing.', src='en', dest='hi')\n",
    "print(translation.text)\n",
    "\n",
    "\n",
    "# blob = TextBlob(u'Something is better than nothing.')\n",
    "# blob.translate(to='hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d6e456",
   "metadata": {},
   "source": [
    "If no source language is specified, **TextBlob** will attempt to detect the language. You can specify the source language explicitly, like so. Raises **TranslatorError** if the TextBlob cannot be translated into the requested language or NotTranslated if the translated result is the same as the input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "476bcdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is always better than not\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "translation = translator.translate('有总比没有好', src='zh-CN', dest='en')\n",
    "print(translation.text)\n",
    "\n",
    "\n",
    "# chinese_blob = TextBlob(u'有总比没有好')\n",
    "# chinese_blob.translate(from_lang=\"zh-CN\", to='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbbaaa5",
   "metadata": {},
   "source": [
    "You can also attempt to detect a TextBlob's language using TextBlob.detect_language()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2716a83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "translator = Translator()\n",
    "text = 'कुछ नहीं से कुछ भला'\n",
    "detection = translator.detect(text)\n",
    "print(detection.lang)\n",
    "\n",
    "# d = TextBlob(u'कुछ नहीं से कुछ भला')\n",
    "# d.detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ef5ba",
   "metadata": {},
   "source": [
    "## TextBlobs are like Python Strings!\n",
    "You can use Python's substring syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2bb33f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Data is a new f\")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c42e8",
   "metadata": {},
   "source": [
    "We can use it as a common string method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1194d151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"DATA IS A NEW FUEL. EXPLICIT IS BETTER THAN IMPLICIT. SIMPLE IS BETTER THAN COMPLEX. \")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8943279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen.find('than') # to show that \"than\" word starts from the 39th place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92cb388",
   "metadata": {},
   "source": [
    "You can make comparisons between TextBlobs and Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6349ada8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_blob=TextBlob('apple')\n",
    "s_blob=TextBlob('samsung')\n",
    "\n",
    "a_blob < s_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "967f4614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_blob=='apple'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf8506a",
   "metadata": {},
   "source": [
    "You can concatenate and interpolate TextBlobs and strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8322b92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"appleandsamsung\")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_blob+ 'and' + s_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8ca05fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple and samsung'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{0} and {1}\".format(a_blob,s_blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50155079",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "The **TextBlob.ngrams()** method returns a list of tuples of n successive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f2202b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Now', 'is', 'better']),\n",
       " WordList(['is', 'better', 'than']),\n",
       " WordList(['better', 'than', 'never'])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob= TextBlob(\"Now is better than never.\")\n",
    "blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85220345",
   "metadata": {},
   "source": [
    "## Get Start and End Indices of Sentences\n",
    "Use sentence.start and sentence.end to get the indices where a sentence starts and ends within a **TextBlob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6dd5bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is a new fuel.\n",
      "---- Starts at index 0, Ends at index 19\n",
      "Explicit is better than implicit.\n",
      "---- Starts at index 20, Ends at index 53\n",
      "Simple is better than complex.\n",
      "---- Starts at index 54, Ends at index 84\n"
     ]
    }
   ],
   "source": [
    "for k in zen.sentences:\n",
    "    print(k)\n",
    "    print(\"---- Starts at index {}, Ends at index {}\".format(k.start, k.end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915b7619",
   "metadata": {},
   "source": [
    "## Let's start building the Text Classification system\n",
    "\n",
    "The textblob.classifiers module makes it simple to create custom classifiers.\n",
    "\n",
    "As an example, let's create a custom sentiment analyzer.\n",
    "\n",
    "### Loading Data and Creating a Classifier\n",
    "\n",
    "First, we’ll create some training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f5fd1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [\n",
    "    ('I love this sandwich.', 'pos'),\n",
    "    ('This is an amazing place!', 'pos'),\n",
    "    ('I feel very good about these beers.', 'pos'),\n",
    "    ('This is my best work.', 'pos'),\n",
    "    ('What an awesome view', 'pos'),\n",
    "    ('I do not like this restaurant', 'neg'),\n",
    "    ('I am tired of this stuff.', 'neg'),\n",
    "    ('I can’t deal with this', 'neg'),\n",
    "    ('He is my sworn enemy!', 'neg'),\n",
    "    ('My boss is horrible.', 'neg')\n",
    "]\n",
    "test = [\n",
    "    ('The beer was good.', 'pos'),\n",
    "    ('I do not enjoy my job', 'neg'),\n",
    "    (\"I ain't feeling dandy today.\", 'neg'),\n",
    "    ('I feel amazing!', 'pos'),\n",
    "    ('Gary is a friend of mine.', 'pos'),\n",
    "    (\"I can't believe I'm doing this.\", 'neg')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb25335",
   "metadata": {},
   "source": [
    "Now we'll create a Naive Bayes classifier, passing the training data into the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6300b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434fb558",
   "metadata": {},
   "source": [
    "## Classifying Text\n",
    "Call the classify(text) method to use the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10afd985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.classify(\"This is an amazing library!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33971e2c",
   "metadata": {},
   "source": [
    "You can get the label probability distribution with the prob_classify(text) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dacb35e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_dist = cl.prob_classify(\"I am suffering from cough and cold.\")\n",
    "prob_dist.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0192dbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(prob_dist.prob(\"neg\"),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d470ccf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(prob_dist.prob(\"pos\"),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c2e5e",
   "metadata": {},
   "source": [
    "## Classifying TextBlobs\n",
    "Another way to classify text is to pass a classifier into the constructor of TextBlob and call its classify() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c51c963d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"Alcohol is good. But the hangover is horrible.\", classifier=cl)\n",
    "blob.classify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4192f501",
   "metadata": {},
   "source": [
    "The advantage of this approach is that you can classify sentences within a TextBlob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f77da09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcohol is good.\n",
      "pos\n",
      "But the hangover is horrible.\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "for b in blob.sentences:\n",
    "    print(b)\n",
    "    print(b.classify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d655848f",
   "metadata": {},
   "source": [
    "## Evaluating Classifiers\n",
    "To compute the accuracy on our test set, use the accuracy(test_data) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b0e17a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3772786",
   "metadata": {},
   "source": [
    "Use the show_informative_features() method to display a listing of the most informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "546aef1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "          contains(this) = True              neg : pos    =      2.3 : 1.0\n",
      "          contains(this) = False             pos : neg    =      1.8 : 1.0\n",
      "          contains(This) = False             neg : pos    =      1.6 : 1.0\n",
      "            contains(an) = False             neg : pos    =      1.6 : 1.0\n",
      "             contains(I) = False             pos : neg    =      1.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "cl.show_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0ae981",
   "metadata": {},
   "source": [
    "## Updating Classifiers with New Data\n",
    "Use the update(new_data) method to update a classifier with new training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1df2e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = [('She is my best friend.', 'pos'),\n",
    "            (\"I'm happy to have a new friend.\", 'pos'),\n",
    "            ('Stay thirsty, my friend.', 'pos'),\n",
    "            (\"He ain't from around here.\", 'neg')]\n",
    "\n",
    "cl.update(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6bd2f831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234b0bd",
   "metadata": {},
   "source": [
    "## Feature Extractors\n",
    "By default, the NaiveBayesClassifier uses a simple feature extractor that indicates which words in the training set are contained in a document.\n",
    "\n",
    "For example, the sentence \"I love\" might have the features contains(love): True or contains(hate): False.\n",
    "\n",
    "You can override this feature extractor by writing your own. A feature extractor is simply a function with a document (the text to extract features from) as the first argument. The function may include a second argument, train_set (the training dataset), if necessary.\n",
    "\n",
    "The function should return a dictionary of features for the document.\n",
    "\n",
    "For example, let’s create a feature extractor that just uses the first and last words of a document as its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a6935be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_word_extractor(document):\n",
    "    tokens = document.split()\n",
    "    first_word, last_word = tokens[0], tokens[-1]\n",
    "    feats = {}\n",
    "    feats[\"first({0})\".format(first_word)] = True\n",
    "    feats[\"last({0})\".format(last_word)] = False\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "89e56f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = end_word_extractor(\"I love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "32b2b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert features == {'last(love)': False, 'first(I)': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc5a24",
   "metadata": {},
   "source": [
    "We can then use the feature extractor in a classifier by passing it as the second argument of the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea5552c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl2 = NaiveBayesClassifier(test, feature_extractor=end_word_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b35843d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"I'm excited to try my new classifier.\", classifier=cl2)\n",
    "blob.classify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f37f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
